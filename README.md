## Описание проекта
Проект посвящен анализу градиентных методов оптимизации для обучения линейных моделей. Рассматриваются GD, SGD и их влияние на скорость сходимости и точность классификации токсичных комментариев из раздела обсуждений английской Википедии

## Датасет
Данные взяты по следующей: [ссылке](https://drive.google.com/open?id=1a-O_gckKyVRQPF5RhBxo-RnZBEGpezE0)

## Решение включает:

1. **Методы оптимизации**:
   - Градиентный спуск (GD) с различными стратегиями обновления весов.
   - Стохастический градиентный спуск (SGD) с разными размерами батча.
   
2. **Эксперименты**:
   - Проверка работы градиентного спуска на синтетических данных.
   - Анализ влияния гиперпараметров на точность и скорость сходимости.
   - Подбор начального приближения.
   - Исследование необходимости обучения bias.
   
3. **Предобработка данных**:
   - Очистка текста от спецсимволов и приведение к нижнему регистру.
   - Лемматизация и удаление стоп-слов.
   - Преобразование текста в векторное представление (Bag of Words, TF-IDF).
   
4. **Анализ ошибок**:
   - Выявление ложноположительных и ложноотрицательных классификаций.
   - Поиск возможных улучшений, включая учет контекста с помощью n-грамм.

## Стек
- Python 3.x
- Pandas, NumPy
- Scikit-learn
- Matplotlib, Seaborn
- NLTK
